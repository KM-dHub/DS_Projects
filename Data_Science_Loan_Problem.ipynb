{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook gives an example of an end-to-end Data Science process (from business problem to evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Problem\n",
    "First National Bank (FNB) Botswana has a dataset about its customers' past loans. Being one of the fastest growing bank in the region, FNB wants detailed insights about customers whose loans are already paid off or defaulted (collection) considering key factors that might have contributed to such loan status. The findings will be very useful as they will enable management and executives to deduce data-driven strategies to develop loan products that will better suit customer needs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the Data\n",
    "To address this problem (which is a classification problem), a Data Scientist is provided with the loan dataset stored in two csv files: __Loan_Train.csv__ and __Loan_Test.csv__. The Loan_Train.csv will be used as training data when building the model and the Loan_Test.csv will be treated as unseen test data used in the model evaluation step.  The dataset includes the following key fields (column attributes):\n",
    "\n",
    "| Field          | Description                                                                           |\n",
    "|----------------|---------------------------------------------------------------------------------------|\n",
    "| Loan_status    | Whether a loan is paid off or is in collection                                           |\n",
    "| Principal      | Basic principal loan amount                                                           |\n",
    "| Terms          | Payoff schedule (frequency)                                                           |\n",
    "| Effective_date | When the loan took effect                                                            |\n",
    "| Due_date       | Payoff schedule due date                                                              |\n",
    "| Age            | Age of applicant                                                                      |\n",
    "| Education      | Education of applicant                                                                |\n",
    "| Gender         | The gender of applicant                                                               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important libraries to get us started!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "print(\"Important libraries to get us started!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('Loan_Train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization and Pre-processing\n",
    "Once loaded, we analyse the dataset to summarize the main characteristics. We perform descriptive statistical analysis to describe our data using e.g., describe(), value_counts(), and info() methods. This allows us to gain more insights about the nature of the data including types, missing data, categorial variables, and whether target classes are balanced or not (to avoid biased model).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing: Feature Selection\n",
    "Important features (column attributes or variables) are selected, and if any, categorial variables are converted to numericals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification: using  scikit-learn library\n",
    "We split the dataset (from __Loan_Train.csv__) into training set and test set to build the model with the best accuracy. We can apply e.g., the following algorithms:\n",
    "- Support Vector Machine (SVM)\n",
    "- Decision Tree\n",
    "- K Nearest Neighbor (KNN)\n",
    "- Logistic Regression\n",
    "\n",
    "For accuracy evaluation, many evaluation scoring metrics can be considered including e.g., root mean square error (RMSE), Jaccard, F1-score and Log-loss (where applicable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data and Standardization\n",
    "Note that we first split data and then perform standardization (on the feature matrix say __X__) to give data zero mean and unit variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Classifiers and Accuracy Scoring Metrics\n",
    "- SVM classifier\n",
    "- Decision Tree classifier\n",
    "- KNN classifier\n",
    "- Logistic Regression classifier\n",
    "- Accuracy scoring metrics (Jaccard, F1-score, Log-Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train (Fit) the Model, Prediction, Accuracy Evaluation\n",
    "Firstly, each classifier is defined, then the model is trained or built, prediction (loan status) is perforned using the\n",
    "developed model, and finally the scoring metrics used to evaluate the accuracy of the model against the test set (__y__ vector)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation and Reporting\n",
    "Repeat only the following steps from above (Loading Data, Feature Selection and Standardization) but this time using __Loan_Test.csv__ dataset. Note that, splitting data and model building are not performed in this step since we are simply testing the already built model (based on the __Loan_Train.csv__ dataset) in the previous steps. It is also worth noting that using unseen test set is important since it validates the robustness of the model post deployment. Finally, report the accuracy of the built model using different evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}